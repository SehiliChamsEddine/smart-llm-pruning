# Resources

Quick links to key transformer and large language model projects:

- **[CS25: Transformers United](https://web.stanford.edu/class/cs25/)** - Stanford course on transformer architectures
- **[Transformer Engine](https://github.com/NVIDIA/TransformerEngine)** - NVIDIA's library for accelerating transformer training
- **[Megatron-LM](https://github.com/NVIDIA/Megatron-LM)** - Large-scale transformer model parallel training
- **[Flash Attention](https://github.com/Dao-AILab/flash-attention)** - Fast and memory-efficient attention implementation
- **[LLM-Pruner](https://github.com/horseee/LLM-Pruner)** - Structural pruning for large language models
